{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader\nfrom transformers import (\n    DistilBertForQuestionAnswering,\n    DistilBertTokenizerFast\n)\nfrom datasets import load_dataset\nimport requests\nimport pickle\nimport base64\nimport gzip\nimport time\n\n\nRANDOM_SEED = 42\nCLIENT_ID   = 1\nSERVER_URL  = \"http://bore.pub:64534\"   # ← paste your URL\n\nEPOCHS_PER_ROUND = 1\nMAX_ROUNDS       = 1\nBATCH_SIZE       = 8\nLR               = 3e-5\nMAX_LENGTH       = 384\nDOC_STRIDE       = 128\n\ntorch.manual_seed(RANDOM_SEED + CLIENT_ID)\nnp.random.seed(RANDOM_SEED + CLIENT_ID)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\" Device: {device}\")\n\nprint(\" Loading SQuAD dataset...\")\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\ndataset   = load_dataset('squad', split='train')\n\n# Client 1 gets first half\nhalf      = len(dataset) // 2\ndataset   = dataset.select(range(0, half))\nprint(f\"Client {CLIENT_ID} — {len(dataset)} examples (first half)\\n\")\n\ndef preprocess(examples):\n    inputs = tokenizer(\n        examples['question'],\n        examples['context'],\n        max_length   = MAX_LENGTH,\n        truncation   = 'only_second',\n        stride       = DOC_STRIDE,\n        return_overflowing_tokens = True,\n        return_offsets_mapping    = True,\n        padding      = 'max_length'\n    )\n\n    offset_mapping   = inputs.pop('offset_mapping')\n    sample_map       = inputs.pop('overflow_to_sample_mapping')\n    answers          = examples['answers']\n    start_positions  = []\n    end_positions    = []\n\n    for i, offset in enumerate(offset_mapping):\n        sample_idx   = sample_map[i]\n        answer       = answers[sample_idx]\n        cls_index    = inputs['input_ids'][i].index(tokenizer.cls_token_id)\n\n        sequence_ids = inputs.sequence_ids(i)\n\n        # If no answer, label CLS token\n        if len(answer['answer_start']) == 0:\n            start_positions.append(cls_index)\n            end_positions.append(cls_index)\n            continue\n\n        start_char = answer['answer_start'][0]\n        end_char   = start_char + len(answer['text'][0])\n\n        # Find token start/end within context\n        token_start = token_end = cls_index\n        for idx, (sid, (o_start, o_end)) in enumerate(zip(sequence_ids, offset)):\n            if sid != 1:\n                continue\n            if o_start <= start_char < o_end:\n                token_start = idx\n            if o_start < end_char <= o_end:\n                token_end = idx\n\n        start_positions.append(token_start)\n        end_positions.append(token_end)\n\n    inputs['start_positions'] = start_positions\n    inputs['end_positions']   = end_positions\n    return inputs\n\nprint(\" Tokenizing dataset (this takes ~2 min)...\")\ntokenized = dataset.map(\n    preprocess,\n    batched=True,\n    remove_columns=dataset.column_names\n)\ntokenized.set_format('torch')\nloader = DataLoader(tokenized, batch_size=BATCH_SIZE, shuffle=True)\nprint(f\"{len(tokenized)} tokenized examples ready\\n\")\n\n\ndef quantize_weights(state_dict):\n    quantized = {}\n    for key, tensor in state_dict.items():\n        w      = tensor.cpu().numpy().flatten().astype(np.float32)\n        w_min  = float(np.min(w))\n        w_max  = float(np.max(w))\n\n        if abs(w_max - w_min) < 1e-8:\n            scale = 1.0; zp = 0.0\n            q     = np.zeros_like(w, dtype=np.int16)\n        else:\n            scale = (w_max - w_min) / 65535.0\n            zp    = w_min\n            q     = np.round((w - w_min) / scale).astype(np.int32) - 32768\n            q     = np.clip(q, -32768, 32767).astype(np.int16)\n\n        quantized[key] = {\n            'quantized'  : base64.b64encode(q.tobytes()).decode('utf-8'),\n            'scale'      : scale,\n            'zero_point' : zp,\n            'shape'      : list(tensor.shape),\n        }\n    return quantized\n\ndef get_size_mb(b):\n    return len(b) / (1024 * 1024)\n\ndef get_global_weights():\n    try:\n        response = requests.get(f\"{SERVER_URL}/get_weights\", timeout=60)\n        if response.status_code != 200:\n            print(f\" Server returned {response.status_code}\")\n            return None, None\n        data       = response.json()\n        compressed = base64.b64decode(data['weights'])\n        pickled    = gzip.decompress(compressed)\n        q_dict     = pickle.loads(pickled)\n\n        # Dequantize\n        state_dict = {}\n        for key, item in q_dict.items():\n            q_bytes    = base64.b64decode(item['quantized'])\n            q_array    = np.frombuffer(q_bytes, dtype=np.int16).copy()\n            dq         = (q_array.astype(np.float32) + 32768) * item['scale'] + item['zero_point']\n            state_dict[key] = torch.tensor(dq.reshape(item['shape']))\n\n        return state_dict, data['round']\n    except Exception as e:\n        print(f\"Error getting weights: {e}\")\n        return None, None\n\ndef submit_weights(state_dict, round_num):\n    try:\n        original_size = get_size_mb(pickle.dumps(\n            {k: v.cpu().numpy() for k, v in state_dict.items()}\n        ))\n\n        print(\"      Quantizing...\")\n        q_dict     = quantize_weights(state_dict)\n        pickled    = pickle.dumps(q_dict)\n        compressed = gzip.compress(pickled, compresslevel=6)\n        encoded    = base64.b64encode(compressed).decode('utf-8')\n        comp_size  = get_size_mb(compressed)\n\n        print(f\"    {original_size:.1f} MB → {comp_size:.1f} MB \"\n              f\"({original_size/comp_size:.1f}x smaller)\")\n\n        payload  = {\n            'client_id'       : CLIENT_ID,\n            'weights'         : encoded,\n            'round'           : round_num,\n            'original_size_mb': original_size,\n            'compressed_size_mb': comp_size,\n        }\n        response = requests.post(\n            f\"{SERVER_URL}/submit_weights\",\n            json=payload, timeout=120\n        )\n\n        if response.status_code != 200:\n            print(f\" Server returned {response.status_code}: {response.text[:200]}\")\n            return None\n        return response.json()\n\n    except Exception as e:\n        print(f\"Error submitting weights: {e}\")\n        import traceback; traceback.print_exc()\n        return None\n\n\nmodel = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased')\nmodel.to(device)\n\nprint(f\"   Federated QA Training — Client {CLIENT_ID}\")\nprint(f\"   Server  : {SERVER_URL}\")\nprint(f\"   Rounds  : {MAX_ROUNDS}  |  Epochs/round: {EPOCHS_PER_ROUND}\\n\")\n\nfor round_num in range(MAX_ROUNDS):\n    print(f\"\\n{'='*60}\")\n    print(f\"ROUND {round_num+1}/{MAX_ROUNDS} — Client {CLIENT_ID}\")\n    print(f\"{'='*60}\")\n\n    # 1. Get global weights\n    print(\"Downloading global model...\")\n    state_dict, server_round = get_global_weights()\n    if state_dict is None:\n        print(\"Failed — retrying in 10s...\")\n        time.sleep(10)\n        continue\n    model.load_state_dict(state_dict)\n    print(\"Global model loaded\")\n\n    # 2. Train locally\n    print(f\"Training ({EPOCHS_PER_ROUND} epochs)...\")\n    model.train()\n    optimizer = AdamW(model.parameters(), lr=LR)\n\n    for epoch in range(EPOCHS_PER_ROUND):\n        total_loss = 0\n        for step, batch in enumerate(loader):\n            batch     = {k: v.to(device) for k, v in batch.items()}\n            outputs   = model(**batch)\n            loss      = outputs.loss\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            total_loss += loss.item()\n\n            if step % 50 == 0:\n                print(f\"   Epoch {epoch+1} | Step {step}/{len(loader)} \"\n                      f\"| Loss: {loss.item():.4f}\")\n\n        avg_loss = total_loss / len(loader)\n        print(f\"Epoch {epoch+1} complete — Avg Loss: {avg_loss:.4f}\")\n\n    # 3. Submit weights\n    print(\"Uploading weights...\")\n    result = submit_weights(model.state_dict(), round_num)\n\n    if result and result.get('status') == 'success':\n        print(\" Weights submitted successfully\")\n        if result.get('aggregating'):\n            print(\"  Server aggregating\")\n    else:\n        print(\"Failed to submit weights\")\n\n    time.sleep(5)\n\nprint(f\"\\n Client {CLIENT_ID} training complete!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-13T14:46:02.539632Z","iopub.execute_input":"2026-02-13T14:46:02.540177Z","iopub.status.idle":"2026-02-13T15:18:00.831437Z","shell.execute_reply.started":"2026-02-13T14:46:02.540140Z","shell.execute_reply":"2026-02-13T15:18:00.830777Z"}},"outputs":[{"name":"stderr","text":"2026-02-13 14:46:15.768177: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1770993975.949841      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1770993976.002173      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1770993976.446472      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770993976.446504      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770993976.446506      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770993976.446509      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":" Device: cuda\n Loading SQuAD dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9531cb95c9bb4a4bae047d4b83391718"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b145bf0508af4560b75e39c45dd6ee2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcd9bc5d2b6344b19ebe9373d53874cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbcba0ecde4e4343bdda9e7e15829a07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a22a1ecfecc143e196345fbe4ff1406a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"plain_text/train-00000-of-00001.parquet:   0%|          | 0.00/14.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ed580f4a6ab40a295f854d3b3259277"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"plain_text/validation-00000-of-00001.par(…):   0%|          | 0.00/1.82M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdbc049dd1a84c9fa1b81218e6d662f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07111edc4719465dae41bfb6570583d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"925ff99254ae48419f6aae3eef5d7999"}},"metadata":{}},{"name":"stdout","text":"Client 1 — 43799 examples (first half)\n\n Tokenizing dataset (this takes ~2 min)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/43799 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd7d42ce7a2348cc84e5bf7d7f6d84e3"}},"metadata":{}},{"name":"stdout","text":"44205 tokenized examples ready\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9ac3c37d85540b791dcb51c1aa2b6d1"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   Federated QA Training — Client 1\n   Server  : http://bore.pub:64534\n   Rounds  : 1  |  Epochs/round: 1\n\n\n============================================================\nROUND 1/1 — Client 1\n============================================================\nDownloading global model...\nGlobal model loaded\nTraining (1 epochs)...\n   Epoch 1 | Step 0/5526 | Loss: 6.1327\n   Epoch 1 | Step 50/5526 | Loss: 3.6640\n   Epoch 1 | Step 100/5526 | Loss: 3.9971\n   Epoch 1 | Step 150/5526 | Loss: 3.1915\n   Epoch 1 | Step 200/5526 | Loss: 2.6102\n   Epoch 1 | Step 250/5526 | Loss: 2.6817\n   Epoch 1 | Step 300/5526 | Loss: 2.3911\n   Epoch 1 | Step 350/5526 | Loss: 2.1128\n   Epoch 1 | Step 400/5526 | Loss: 2.1841\n   Epoch 1 | Step 450/5526 | Loss: 1.6781\n   Epoch 1 | Step 500/5526 | Loss: 1.3145\n   Epoch 1 | Step 550/5526 | Loss: 2.8394\n   Epoch 1 | Step 600/5526 | Loss: 0.8925\n   Epoch 1 | Step 650/5526 | Loss: 1.6221\n   Epoch 1 | Step 700/5526 | Loss: 1.8400\n   Epoch 1 | Step 750/5526 | Loss: 1.5115\n   Epoch 1 | Step 800/5526 | Loss: 1.9662\n   Epoch 1 | Step 850/5526 | Loss: 1.9020\n   Epoch 1 | Step 900/5526 | Loss: 2.0755\n   Epoch 1 | Step 950/5526 | Loss: 1.1824\n   Epoch 1 | Step 1000/5526 | Loss: 0.9015\n   Epoch 1 | Step 1050/5526 | Loss: 2.0698\n   Epoch 1 | Step 1100/5526 | Loss: 1.2686\n   Epoch 1 | Step 1150/5526 | Loss: 1.8198\n   Epoch 1 | Step 1200/5526 | Loss: 1.6786\n   Epoch 1 | Step 1250/5526 | Loss: 1.1250\n   Epoch 1 | Step 1300/5526 | Loss: 1.5514\n   Epoch 1 | Step 1350/5526 | Loss: 2.0657\n   Epoch 1 | Step 1400/5526 | Loss: 1.6634\n   Epoch 1 | Step 1450/5526 | Loss: 1.1729\n   Epoch 1 | Step 1500/5526 | Loss: 1.2017\n   Epoch 1 | Step 1550/5526 | Loss: 1.1816\n   Epoch 1 | Step 1600/5526 | Loss: 1.4860\n   Epoch 1 | Step 1650/5526 | Loss: 0.7748\n   Epoch 1 | Step 1700/5526 | Loss: 1.3114\n   Epoch 1 | Step 1750/5526 | Loss: 2.0496\n   Epoch 1 | Step 1800/5526 | Loss: 1.4678\n   Epoch 1 | Step 1850/5526 | Loss: 1.3037\n   Epoch 1 | Step 1900/5526 | Loss: 1.5647\n   Epoch 1 | Step 1950/5526 | Loss: 1.5112\n   Epoch 1 | Step 2000/5526 | Loss: 1.1298\n   Epoch 1 | Step 2050/5526 | Loss: 1.8987\n   Epoch 1 | Step 2100/5526 | Loss: 0.9351\n   Epoch 1 | Step 2150/5526 | Loss: 1.3755\n   Epoch 1 | Step 2200/5526 | Loss: 2.0512\n   Epoch 1 | Step 2250/5526 | Loss: 1.1814\n   Epoch 1 | Step 2300/5526 | Loss: 1.5314\n   Epoch 1 | Step 2350/5526 | Loss: 2.2012\n   Epoch 1 | Step 2400/5526 | Loss: 1.2695\n   Epoch 1 | Step 2450/5526 | Loss: 1.0473\n   Epoch 1 | Step 2500/5526 | Loss: 0.8537\n   Epoch 1 | Step 2550/5526 | Loss: 1.2238\n   Epoch 1 | Step 2600/5526 | Loss: 1.1618\n   Epoch 1 | Step 2650/5526 | Loss: 2.5564\n   Epoch 1 | Step 2700/5526 | Loss: 0.4402\n   Epoch 1 | Step 2750/5526 | Loss: 2.6266\n   Epoch 1 | Step 2800/5526 | Loss: 1.1123\n   Epoch 1 | Step 2850/5526 | Loss: 1.6435\n   Epoch 1 | Step 2900/5526 | Loss: 1.4653\n   Epoch 1 | Step 2950/5526 | Loss: 1.1010\n   Epoch 1 | Step 3000/5526 | Loss: 0.6956\n   Epoch 1 | Step 3050/5526 | Loss: 1.0533\n   Epoch 1 | Step 3100/5526 | Loss: 0.4022\n   Epoch 1 | Step 3150/5526 | Loss: 1.5190\n   Epoch 1 | Step 3200/5526 | Loss: 1.1801\n   Epoch 1 | Step 3250/5526 | Loss: 0.4404\n   Epoch 1 | Step 3300/5526 | Loss: 0.5156\n   Epoch 1 | Step 3350/5526 | Loss: 0.9227\n   Epoch 1 | Step 3400/5526 | Loss: 1.4085\n   Epoch 1 | Step 3450/5526 | Loss: 1.6526\n   Epoch 1 | Step 3500/5526 | Loss: 0.6834\n   Epoch 1 | Step 3550/5526 | Loss: 1.9961\n   Epoch 1 | Step 3600/5526 | Loss: 0.9270\n   Epoch 1 | Step 3650/5526 | Loss: 1.6975\n   Epoch 1 | Step 3700/5526 | Loss: 0.3721\n   Epoch 1 | Step 3750/5526 | Loss: 1.2225\n   Epoch 1 | Step 3800/5526 | Loss: 1.4026\n   Epoch 1 | Step 3850/5526 | Loss: 0.8389\n   Epoch 1 | Step 3900/5526 | Loss: 1.3159\n   Epoch 1 | Step 3950/5526 | Loss: 0.8440\n   Epoch 1 | Step 4000/5526 | Loss: 0.9529\n   Epoch 1 | Step 4050/5526 | Loss: 0.9646\n   Epoch 1 | Step 4100/5526 | Loss: 1.5739\n   Epoch 1 | Step 4150/5526 | Loss: 0.9826\n   Epoch 1 | Step 4200/5526 | Loss: 0.6014\n   Epoch 1 | Step 4250/5526 | Loss: 1.4807\n   Epoch 1 | Step 4300/5526 | Loss: 1.1898\n   Epoch 1 | Step 4350/5526 | Loss: 0.9612\n   Epoch 1 | Step 4400/5526 | Loss: 2.3897\n   Epoch 1 | Step 4450/5526 | Loss: 2.0471\n   Epoch 1 | Step 4500/5526 | Loss: 1.0486\n   Epoch 1 | Step 4550/5526 | Loss: 1.5691\n   Epoch 1 | Step 4600/5526 | Loss: 1.6558\n   Epoch 1 | Step 4650/5526 | Loss: 0.7078\n   Epoch 1 | Step 4700/5526 | Loss: 1.4523\n   Epoch 1 | Step 4750/5526 | Loss: 1.0567\n   Epoch 1 | Step 4800/5526 | Loss: 1.9046\n   Epoch 1 | Step 4850/5526 | Loss: 0.5744\n   Epoch 1 | Step 4900/5526 | Loss: 1.2105\n   Epoch 1 | Step 4950/5526 | Loss: 0.8056\n   Epoch 1 | Step 5000/5526 | Loss: 1.1946\n   Epoch 1 | Step 5050/5526 | Loss: 1.0985\n   Epoch 1 | Step 5100/5526 | Loss: 0.7440\n   Epoch 1 | Step 5150/5526 | Loss: 0.4659\n   Epoch 1 | Step 5200/5526 | Loss: 1.0286\n   Epoch 1 | Step 5250/5526 | Loss: 1.1194\n   Epoch 1 | Step 5300/5526 | Loss: 0.8648\n   Epoch 1 | Step 5350/5526 | Loss: 0.6788\n   Epoch 1 | Step 5400/5526 | Loss: 2.9671\n   Epoch 1 | Step 5450/5526 | Loss: 0.8973\n   Epoch 1 | Step 5500/5526 | Loss: 0.7459\nEpoch 1 complete — Avg Loss: 1.4660\nUploading weights...\n      Quantizing...\n    253.2 MB → 124.0 MB (2.0x smaller)\n Weights submitted successfully\n  Server aggregating\n\n Client 1 training complete!\n","output_type":"stream"}],"execution_count":1}]}