{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install flask flask-cors\n",
        "!wget -q https://github.com/ekzhang/bore/releases/download/v0.5.0/bore-v0.5.0-x86_64-unknown-linux-musl.tar.gz\n",
        "!tar -xzf bore-v0.5.0-x86_64-unknown-linux-musl.tar.gz\n",
        "!chmod +x bore\n",
        "!mv bore /usr/local/bin/\n",
        "\n"
      ],
      "metadata": {
        "id": "5udO2WETbkiS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66f6e5a7-74d6-4d7d-b1e6-458926040501"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flask in /usr/local/lib/python3.12/dist-packages (3.1.2)\n",
            "Requirement already satisfied: flask-cors in /usr/local/lib/python3.12/dist-packages (6.0.2)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.12/dist-packages (from flask) (8.3.1)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from flask) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from flask) (3.0.3)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from flask) (3.1.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Fedrated Learning fine tuning"
      ],
      "metadata": {
        "id": "OXcGZZus9VT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets torch -q"
      ],
      "metadata": {
        "id": "JmD_JqG09bVh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from flask import Flask, request, jsonify\n",
        "import pickle\n",
        "import base64\n",
        "import threading\n",
        "import subprocess\n",
        "import time\n",
        "import os\n",
        "import gzip\n",
        "RANDOM_SEED = 42\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "num_clients    = 2\n",
        "current_round  = 0\n",
        "max_rounds     = 1\n",
        "client_weights = {}\n",
        "\n",
        "app = Flask(__name__)\n",
        "app.config['MAX_CONTENT_LENGTH'] = 512 * 1024 * 1024  # 512 MB\n",
        "\n",
        "def load_distilbert():\n",
        "    from transformers import DistilBertForQuestionAnswering\n",
        "    model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased')\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "print(\" Initializing DistilBERT for Question Answering...\")\n",
        "global_model = load_distilbert()\n",
        "print(f\" Model ready — {sum(p.numel() for p in global_model.parameters()):,} parameters\")\n",
        "\n",
        "def get_model_weights():\n",
        "    \"\"\"Extract state dict as numpy arrays — no HuggingFace classes involved\"\"\"\n",
        "    return {k: v.cpu().detach().numpy() for k, v in global_model.state_dict().items()}\n",
        "\n",
        "def set_model_weights(weights_dict):\n",
        "    \"\"\"Load numpy arrays back into model\"\"\"\n",
        "    state_dict = {k: torch.tensor(v) for k, v in weights_dict.items()}\n",
        "    global_model.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "global_weights = get_model_weights()\n",
        "print(\"Global weights extracted!\")\n",
        "\n",
        "def quantize_weights(weights_dict):\n",
        "    \"\"\"Quantize float32 weight dict → INT16\"\"\"\n",
        "    quantized = {}\n",
        "    for key, w in weights_dict.items():\n",
        "        w_flat = w.flatten().astype(np.float32)\n",
        "        w_min  = float(np.min(w_flat))\n",
        "        w_max  = float(np.max(w_flat))\n",
        "\n",
        "        if abs(w_max - w_min) < 1e-8:\n",
        "            scale = 1.0\n",
        "            zp    = 0.0\n",
        "            q     = np.zeros_like(w_flat, dtype=np.int16)\n",
        "        else:\n",
        "            scale = (w_max - w_min) / 65535.0\n",
        "            zp    = w_min\n",
        "            q     = np.round((w_flat - w_min) / scale).astype(np.int32) - 32768\n",
        "            q     = np.clip(q, -32768, 32767).astype(np.int16)\n",
        "\n",
        "        quantized[key] = {\n",
        "            'quantized'  : base64.b64encode(q.tobytes()).decode('utf-8'),\n",
        "            'scale'      : float(scale),\n",
        "            'zero_point' : float(zp),\n",
        "            'shape'      : list(w.shape),\n",
        "        }\n",
        "    return quantized\n",
        "\n",
        "def dequantize_weights(quantized_dict):\n",
        "    \"\"\"Dequantize INT16 weight dict → float32 numpy arrays\"\"\"\n",
        "    weights = {}\n",
        "    for key, item in quantized_dict.items():\n",
        "        q_bytes  = base64.b64decode(item['quantized'])\n",
        "        q_array  = np.frombuffer(q_bytes, dtype=np.int16).copy()\n",
        "        scale    = float(item['scale'])\n",
        "        zp       = float(item['zero_point'])\n",
        "        shape    = tuple(item['shape'])\n",
        "        dq       = (q_array.astype(np.float32) + 32768) * scale + zp\n",
        "        weights[key] = dq.reshape(shape)\n",
        "    return weights\n",
        "\n",
        "def aggregate_weights():\n",
        "    global global_weights, client_weights\n",
        "\n",
        "    n        = len(client_weights)\n",
        "    averaged = {k: np.zeros_like(v) for k, v in global_weights.items()}\n",
        "\n",
        "    for weights in client_weights.values():\n",
        "        for k in averaged:\n",
        "            averaged[k] += weights[k]\n",
        "\n",
        "    global_weights = {k: v / n for k, v in averaged.items()}\n",
        "    set_model_weights(global_weights)\n",
        "    print(\"Global model updated with averaged weights\")\n",
        "\n",
        "@app.route('/get_weights', methods=['GET'])\n",
        "def get_weights():\n",
        "\n",
        "    quantized  = quantize_weights(global_weights)\n",
        "    pickled    = pickle.dumps(quantized)\n",
        "    compressed = gzip.compress(pickled, compresslevel=6)\n",
        "    encoded    = base64.b64encode(compressed).decode('utf-8')\n",
        "\n",
        "    return jsonify({\n",
        "        'weights' : encoded,\n",
        "        'round'   : current_round,\n",
        "        'status'  : 'success'\n",
        "    })\n",
        "\n",
        "@app.route('/submit_weights', methods=['POST'])\n",
        "def submit_weights():\n",
        "    global client_weights, global_weights, current_round\n",
        "\n",
        "    if not request.is_json:\n",
        "        return jsonify({'status': 'error', 'message': 'Content-Type must be application/json'}), 415\n",
        "\n",
        "    data = request.get_json(force=True, silent=True)\n",
        "    if data is None:\n",
        "        return jsonify({'status': 'error', 'message': 'Failed to parse JSON body'}), 400\n",
        "\n",
        "    client_id       = data.get('client_id')\n",
        "    encoded_weights = data.get('weights')\n",
        "    client_round    = data.get('round', 0)\n",
        "    original_size   = data.get('original_size_mb', 0)\n",
        "    compressed_size = data.get('compressed_size_mb', 0)\n",
        "\n",
        "    if client_id is None or encoded_weights is None:\n",
        "        return jsonify({'status': 'error', 'message': 'Missing client_id or weights'}), 400\n",
        "\n",
        "    try:\n",
        "        compressed     = base64.b64decode(encoded_weights)\n",
        "        pickled        = gzip.decompress(compressed)\n",
        "        quantized_dict = pickle.loads(pickled)\n",
        "        weights        = dequantize_weights(quantized_dict)\n",
        "    except Exception as e:\n",
        "        import traceback; traceback.print_exc()\n",
        "        return jsonify({'status': 'error', 'message': str(e)}), 500\n",
        "\n",
        "    client_weights[client_id] = weights\n",
        "\n",
        "    ratio = original_size / compressed_size if compressed_size > 0 else 0\n",
        "    print(f\" Client {client_id} | Round {client_round} | \"\n",
        "          f\"{original_size:.1f}MB → {compressed_size:.1f}MB ({ratio:.1f}x) | \"\n",
        "          f\"{len(client_weights)}/{num_clients} clients\")\n",
        "\n",
        "    should_aggregate = (len(client_weights) == num_clients)\n",
        "\n",
        "    response = jsonify({\n",
        "        'status'            : 'success',\n",
        "        'message'           : f'Weights received from client {client_id}',\n",
        "        'clients_submitted' : len(client_weights),\n",
        "        'total_clients'     : num_clients,\n",
        "        'aggregating'       : should_aggregate\n",
        "    })\n",
        "\n",
        "    if should_aggregate:\n",
        "        def do_aggregate():\n",
        "            global global_weights, client_weights, current_round\n",
        "            print(f\"\\n Aggregating round {current_round + 1}...\")\n",
        "            aggregate_weights()\n",
        "            current_round += 1\n",
        "            client_weights = {}\n",
        "            print(f\"✓ Round {current_round} complete!\\n\")\n",
        "\n",
        "            if current_round >= max_rounds:\n",
        "                print(\" TRAINING COMPLETE!\")\n",
        "                save_final_model()\n",
        "\n",
        "        threading.Thread(target=do_aggregate, daemon=True).start()\n",
        "\n",
        "    return response\n",
        "\n",
        "@app.route('/status', methods=['GET'])\n",
        "def status():\n",
        "    return jsonify({\n",
        "        'status'            : 'running',\n",
        "        'current_round'     : current_round,\n",
        "        'max_rounds'        : max_rounds,\n",
        "        'clients_submitted' : len(client_weights),\n",
        "        'total_clients'     : num_clients\n",
        "    })\n",
        "\n",
        "def save_final_model():\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"SAVING FINAL MODEL...\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    try:\n",
        "        # FIX: import inside function — keeps pickle namespace clean\n",
        "        from transformers import DistilBertForQuestionAnswering\n",
        "\n",
        "        os.makedirs('federated_qa_model', exist_ok=True)\n",
        "        global_model.save_pretrained('federated_qa_model')\n",
        "        print(\"Saved to 'federated_qa_model/'\")\n",
        "\n",
        "        torch.save(global_model.state_dict(), 'federated_qa_model.pt')\n",
        "        print(\"Saved as 'federated_qa_model.pt'\")\n",
        "\n",
        "        total_params = sum(p.numel() for p in global_model.parameters())\n",
        "        size_mb      = os.path.getsize('federated_qa_model.pt') / (1024 * 1024)\n",
        "        print(f\"\\n Parameters : {total_params:,}\")\n",
        "        print(f\" Size       : {size_mb:.2f} MB\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Error saving: {e}\")\n",
        "        import traceback; traceback.print_exc()\n",
        "\n",
        "def run_flask():\n",
        "    app.run(host='0.0.0.0', port=5000, threaded=True, use_reloader=False)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    subprocess.run(['pkill', '-f', 'bore'], capture_output=True)\n",
        "    time.sleep(1)\n",
        "\n",
        "    flask_thread = threading.Thread(target=run_flask, daemon=True)\n",
        "    flask_thread.start()\n",
        "    time.sleep(2)\n",
        "    print(\"Flask started on port 5000\\n\")\n",
        "\n",
        "    bore_process = subprocess.Popen(\n",
        "        ['bore', 'local', '5000', '--to', 'bore.pub'],\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        text=True, bufsize=1\n",
        "    )\n",
        "\n",
        "    for line in bore_process.stdout:\n",
        "        print(line.rstrip())\n",
        "        if 'bore.pub:' in line:\n",
        "            try:\n",
        "                port = line.split('bore.pub:')[1].split()[0].strip().rstrip('/')\n",
        "                url  = f\"http://bore.pub:{port}\"\n",
        "                print(\"\\n\" + \"=\"*60)\n",
        "                print(f\"SERVER URL: {url}\")\n",
        "                print(\"=\"*60)\n",
        "                print(f\"Server ready — waiting for {num_clients} clients\")\n",
        "                print(f\"Training: {max_rounds} rounds\\n\")\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    bore_process.wait()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f154eedc0d6c45b682cd69240c8f7cef",
            "e59796799fed4a45a12c41826da2cb89",
            "38d89363cbc046a39986e6d39a0255d5",
            "91bd142063334108a6e1d867407ce61c",
            "6a7f76c0506c440fbbd94714f36a7fa8",
            "ae9c795047e84d439bf68a958ab7b2d9",
            "e04c0962e0924ef5b4545d1d35122240",
            "84f02b3979d34c7ca8c0287eb4bfa8f8",
            "643fc37729774bef94c9965705dbe048",
            "715fc17f482f43bfa76205dedc3ad42c",
            "955a96ac454440609e015ba8414d55af",
            "62232d6733e24e8d981e36a5fd9870c1",
            "ae232d49bea2446f8d29482c256f6f90",
            "1a0f3d2b549247d7a1165914197ffc89",
            "677e41d8fb1940faa98b3f0488234d9a",
            "e42da491b2ad47a0b45b3b7236bc2037",
            "e2e9d7e48838454ab71d2d98c3a0f59e",
            "6dafecd663a246bbaab28aa7aa95d801",
            "9926b64da95d46e0b68b77011213aa52",
            "1be8726bbacc4bb8a9b026f7aabc9bd3",
            "d3bdaf3e6d11452f94244970cc464bbf",
            "099dc0e752d24141aa790635707fce7a"
          ]
        },
        "id": "Domo9yj-9gwt",
        "outputId": "40785dca-3a2e-4a76-e997-31a58a9ed081"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Initializing DistilBERT for Question Answering...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f154eedc0d6c45b682cd69240c8f7cef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DistilBertForQuestionAnswering LOAD REPORT from: distilbert-base-uncased\n",
            "Key                     | Status     | \n",
            "------------------------+------------+-\n",
            "vocab_transform.weight  | UNEXPECTED | \n",
            "vocab_transform.bias    | UNEXPECTED | \n",
            "vocab_projector.bias    | UNEXPECTED | \n",
            "vocab_layer_norm.weight | UNEXPECTED | \n",
            "vocab_layer_norm.bias   | UNEXPECTED | \n",
            "qa_outputs.bias         | MISSING    | \n",
            "qa_outputs.weight       | MISSING    | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
            "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Model ready — 66,364,418 parameters\n",
            "Global weights extracted!\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5000\n",
            " * Running on http://172.28.0.12:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flask started on port 5000\n",
            "\n",
            "\u001b[2m2026-02-13T14:45:27.576629Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mbore_cli::client\u001b[0m\u001b[2m:\u001b[0m connected to server \u001b[3mremote_port\u001b[0m\u001b[2m=\u001b[0m64534\n",
            "\u001b[2m2026-02-13T14:45:27.577115Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mbore_cli::client\u001b[0m\u001b[2m:\u001b[0m listening at bore.pub:64534\n",
            "\n",
            "============================================================\n",
            "SERVER URL: http://bore.pub:64534\n",
            "============================================================\n",
            "Server ready — waiting for 2 clients\n",
            "Training: 1 rounds\n",
            "\n",
            "\u001b[2m2026-02-13T14:46:55.108356Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mproxy\u001b[0m\u001b[1m{\u001b[0m\u001b[3mid\u001b[0m\u001b[2m=\u001b[0ma8cd2eb3-4d86-4696-be64-29291eeb234b\u001b[1m}\u001b[0m\u001b[2m:\u001b[0m \u001b[2mbore_cli::client\u001b[0m\u001b[2m:\u001b[0m new connection\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [13/Feb/2026 14:47:17] \"GET /get_weights HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m2026-02-13T14:47:21.872582Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mproxy\u001b[0m\u001b[1m{\u001b[0m\u001b[3mid\u001b[0m\u001b[2m=\u001b[0ma8cd2eb3-4d86-4696-be64-29291eeb234b\u001b[1m}\u001b[0m\u001b[2m:\u001b[0m \u001b[2mbore_cli::client\u001b[0m\u001b[2m:\u001b[0m connection exited\n",
            "\u001b[2m2026-02-13T14:47:24.741862Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mproxy\u001b[0m\u001b[1m{\u001b[0m\u001b[3mid\u001b[0m\u001b[2m=\u001b[0mc02e1c8f-75ea-4657-a06d-262a27a87893\u001b[1m}\u001b[0m\u001b[2m:\u001b[0m \u001b[2mbore_cli::client\u001b[0m\u001b[2m:\u001b[0m new connection\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [13/Feb/2026 14:47:40] \"GET /get_weights HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m2026-02-13T14:47:44.765239Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mproxy\u001b[0m\u001b[1m{\u001b[0m\u001b[3mid\u001b[0m\u001b[2m=\u001b[0mc02e1c8f-75ea-4657-a06d-262a27a87893\u001b[1m}\u001b[0m\u001b[2m:\u001b[0m \u001b[2mbore_cli::client\u001b[0m\u001b[2m:\u001b[0m connection exited\n",
            "\u001b[2m2026-02-13T15:16:15.264620Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mproxy\u001b[0m\u001b[1m{\u001b[0m\u001b[3mid\u001b[0m\u001b[2m=\u001b[0m3999d346-947d-46fb-8318-87a2434a4279\u001b[1m}\u001b[0m\u001b[2m:\u001b[0m \u001b[2mbore_cli::client\u001b[0m\u001b[2m:\u001b[0m new connection\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [13/Feb/2026 15:16:27] \"POST /submit_weights HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Client 2 | Round 0 | 253.2MB → 124.0MB (2.0x) | 1/2 clients\n",
            "\u001b[2m2026-02-13T15:16:28.001875Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mproxy\u001b[0m\u001b[1m{\u001b[0m\u001b[3mid\u001b[0m\u001b[2m=\u001b[0m3999d346-947d-46fb-8318-87a2434a4279\u001b[1m}\u001b[0m\u001b[2m:\u001b[0m \u001b[2mbore_cli::client\u001b[0m\u001b[2m:\u001b[0m connection exited\n",
            "\u001b[2m2026-02-13T15:17:42.686032Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mproxy\u001b[0m\u001b[1m{\u001b[0m\u001b[3mid\u001b[0m\u001b[2m=\u001b[0m3e132d93-e6d6-4156-a69d-9a208c7e9ee7\u001b[1m}\u001b[0m\u001b[2m:\u001b[0m \u001b[2mbore_cli::client\u001b[0m\u001b[2m:\u001b[0m new connection\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [13/Feb/2026 15:17:55] \"POST /submit_weights HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Client 1 | Round 0 | 253.2MB → 124.0MB (2.0x) | 2/2 clients\n",
            "\n",
            " Aggregating round 1...\n",
            "\u001b[2m2026-02-13T15:17:55.736697Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mproxy\u001b[0m\u001b[1m{\u001b[0m\u001b[3mid\u001b[0m\u001b[2m=\u001b[0m3e132d93-e6d6-4156-a69d-9a208c7e9ee7\u001b[1m}\u001b[0m\u001b[2m:\u001b[0m \u001b[2mbore_cli::client\u001b[0m\u001b[2m:\u001b[0m connection exited\n",
            "Global model updated with averaged weights\n",
            "✓ Round 1 complete!\n",
            "\n",
            " TRAINING COMPLETE!\n",
            "\n",
            "============================================================\n",
            "SAVING FINAL MODEL...\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "62232d6733e24e8d981e36a5fd9870c1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved to 'federated_qa_model/'\n",
            "Saved as 'federated_qa_model.pt'\n",
            "\n",
            " Parameters : 66,364,418\n",
            " Size       : 253.20 MB\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1724299889.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    228\u001b[0m     )\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbore_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'bore.pub:'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "from transformers import DistilBertForQuestionAnswering\n",
        "\n",
        "print(\"Loading trained federated model...\")\n",
        "model = DistilBertForQuestionAnswering.from_pretrained('federated_qa_model')\n",
        "model.eval()\n",
        "\n",
        "original_size   = os.path.getsize('federated_qa_model.pt') / (1024 * 1024)\n",
        "original_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Model loaded!\")\n",
        "print(f\"Parameters : {original_params:,}\")\n",
        "print(f\"Size: {original_size:.2f} MB\\n\")\n",
        "\n",
        "results = {}\n",
        "\n",
        "print(\"─\"*55)\n",
        "print(\"Dynamic INT8 Quantization\")\n",
        "print(\"─\"*55)\n",
        "try:\n",
        "    dynamic_model = torch.quantization.quantize_dynamic(\n",
        "        model,\n",
        "        {torch.nn.Linear},\n",
        "        dtype=torch.qint8\n",
        "    )\n",
        "    path = 'model_dynamicc_int8.pt'\n",
        "    torch.save(dynamic_model.state_dict(), path)\n",
        "    size = os.path.getsize(path) / (1024 * 1024)\n",
        "    results['dynamic'] = {'path': path, 'size': size}\n",
        "    print(f\"Saved : {path}\")\n",
        "    print(f\"Size  : {size:.2f} MB  ({original_size/size:.1f}x smaller)\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed: {e}\\n\")\n",
        "print(\"─\"*55)\n",
        "print(\"ONNX Export (float32)\")\n",
        "print(\"─\"*55)\n",
        "try:\n",
        "    dummy_ids  = torch.zeros(1, 384, dtype=torch.long)\n",
        "    dummy_mask = torch.zeros(1, 384, dtype=torch.long)\n",
        "\n",
        "    torch.onnx.export(\n",
        "        model,\n",
        "        (dummy_ids, dummy_mask),\n",
        "        'model_float32.onnx',\n",
        "        input_names   = ['input_ids', 'attention_mask'],\n",
        "        output_names  = ['start_logits', 'end_logits'],\n",
        "        dynamic_axes  = {\n",
        "            'input_ids'     : {0: 'batch', 1: 'seq'},\n",
        "            'attention_mask': {0: 'batch', 1: 'seq'},\n",
        "            'start_logits'  : {0: 'batch'},\n",
        "            'end_logits'    : {0: 'batch'}\n",
        "        },\n",
        "        opset_version = 13\n",
        "    )\n",
        "    size = os.path.getsize('model_float32.onnx') / (1024 * 1024)\n",
        "    results['onnx'] = {'path': 'model_float32.onnx', 'size': size}\n",
        "    print(f\"Saved : model_float32.onnx\")\n",
        "    print(f\"Size  : {size:.2f} MB  ({original_size/size:.1f}x smaller)\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed: {e}\\n\")\n",
        "\n",
        "\n",
        "print(\"─\"*55)\n",
        "print(\"ONNX + INT8 Quantization\")\n",
        "print(\"─\"*55)\n",
        "try:\n",
        "    from onnxruntime.quantization import quantize_dynamic, QuantType\n",
        "\n",
        "    quantize_dynamic(\n",
        "        'model_float32.onnx',\n",
        "        'model_int8.onnx',\n",
        "        weight_type = QuantType.QInt8\n",
        "    )\n",
        "    size = os.path.getsize('model_int8.onnx') / (1024 * 1024)\n",
        "    results['onnx_int8'] = {'path': 'model_int8.onnx', 'size': size}\n",
        "    print(f\"Saved : model_int8.onnx\")\n",
        "    print(f\"Size  : {size:.2f} MB  ({original_size/size:.1f}x smaller)\\n\")\n",
        "except ImportError:\n",
        "    print(\"Run: !pip install onnxruntime -q  then re-run this cell\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed: {e}\\n\")\n",
        "\n",
        "\n",
        "label_map = {\n",
        "    'dynamic'  : 'Dynamic INT8 (PyTorch)',\n",
        "    'onnx'     : 'ONNX float32',\n",
        "    'onnx_int8': 'ONNX INT8',\n",
        "}\n",
        "\n",
        "print(\"=\"*55)\n",
        "print(\"COMPRESSION SUMMARY\")\n",
        "print(\"=\"*55)\n",
        "print(f\"  {'Model':<28} {'Size':>9}  {'Reduction':>10}\")\n",
        "print(f\"  {'-'*50}\")\n",
        "print(f\"  {'Original (float32 .pt)':<28} {original_size:>8.2f}MB  {'1.0x':>10}\")\n",
        "for k, label in label_map.items():\n",
        "    if k in results:\n",
        "        r = results[k]\n",
        "        print(f\"  {label:<28} {r['size']:>8.2f}MB  {original_size/r['size']:>9.1f}x\")\n",
        "print(\"=\"*55)\n",
        "\n",
        "print(\"\\n Downloading all files...\")\n",
        "download_list = [\n",
        "    ('federated_qa_model.pt', 'Original model'),\n",
        "    *[(r['path'], label_map[k]) for k, r in results.items()]\n",
        "]\n",
        "\n",
        "for path, label in download_list:\n",
        "    if os.path.exists(path):\n",
        "        size = os.path.getsize(path) / (1024 * 1024)\n",
        "        print(f\" {label:<28} ({size:.2f} MB)\")\n",
        "        files.download(path)\n",
        "    else:\n",
        "        print(f\"{label:<28} not found\")\n",
        "\n",
        "print(\"\\n Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 798,
          "referenced_widgets": [
            "306afe6708d64071b597ac28721692d9",
            "589d73181cdd44f7aa0f9e117a591c8b",
            "efb0ef4eee414bf0a023286b6ae967a8",
            "bd44225c1f9846c99d67df807ff5bd8c",
            "0086c76400014f2994fb77146494575c",
            "b04cd112d5fa41a19d20e7eeb6bec906",
            "60bd86cbad2a4e12b6fe95782c4f744d",
            "e1fcda733624480da9630daeeba7bf1d",
            "cacc447c4b1941d6b825104e7c3fd7ef",
            "0938aced824c4e6083205fd3cf113a99",
            "dfdb7a76227446eeaa7d1867873357cd"
          ]
        },
        "id": "grdKEEWsPaPQ",
        "outputId": "7a151947-6d2a-4c1a-d041-800a7ec6a42a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading trained federated model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/102 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "306afe6708d64071b597ac28721692d9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded!\n",
            "Parameters : 66,364,418\n",
            "Size: 253.20 MB\n",
            "\n",
            "───────────────────────────────────────────────────────\n",
            "Dynamic INT8 Quantization\n",
            "───────────────────────────────────────────────────────\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1597874469.py:23: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  dynamic_model = torch.quantization.quantize_dynamic(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved : model_dynamicc_int8.pt\n",
            "Size  : 131.72 MB  (1.9x smaller)\n",
            "\n",
            "───────────────────────────────────────────────────────\n",
            "ONNX Export (float32)\n",
            "───────────────────────────────────────────────────────\n",
            "Failed: No module named 'onnxscript'\n",
            "\n",
            "───────────────────────────────────────────────────────\n",
            "ONNX + INT8 Quantization\n",
            "───────────────────────────────────────────────────────\n",
            "Run: !pip install onnxruntime -q  then re-run this cell\n",
            "\n",
            "=======================================================\n",
            "COMPRESSION SUMMARY\n",
            "=======================================================\n",
            "  Model                             Size   Reduction\n",
            "  --------------------------------------------------\n",
            "  Original (float32 .pt)         253.20MB        1.0x\n",
            "  Dynamic INT8 (PyTorch)         131.72MB        1.9x\n",
            "=======================================================\n",
            "\n",
            " Downloading all files...\n",
            " Original model               (253.20 MB)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6a286516-cee3-4f98-b1d3-dbd91b6bf267\", \"federated_qa_model.pt\", 265498647)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Dynamic INT8 (PyTorch)       (131.72 MB)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_45ba13da-a226-4904-bbf4-9ce8b0876290\", \"model_dynamicc_int8.pt\", 138122535)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import DistilBertForQuestionAnswering, DistilBertTokenizerFast\n",
        "\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "context = \"\"\"\n",
        "Maria owns a small bakery in downtown Portland. She opens at 6 AM every morning\n",
        "and bakes fresh bread, croissants, and cookies. Her specialty is sourdough bread,\n",
        "which takes 24 hours to prepare. The bakery closes at 5 PM on weekdays and\n",
        "3 PM on Sundays.\n",
        "\"\"\"\n",
        "\n",
        "question = \"What is Maria's specialty at the bakery?\"\n",
        "\n",
        "inputs = tokenizer(\n",
        "    question,\n",
        "    context,\n",
        "    return_tensors='pt',\n",
        "    max_length=384,\n",
        "    truncation='only_second',\n",
        "    padding='max_length'\n",
        ")\n",
        "\n",
        "def get_answer(model, inputs):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        start_idx = torch.argmax(outputs.start_logits)\n",
        "        end_idx = torch.argmax(outputs.end_logits)\n",
        "        tokens = inputs['input_ids'][0][start_idx:end_idx + 1]\n",
        "        answer = tokenizer.decode(tokens, skip_special_tokens=True)\n",
        "    return answer\n",
        "\n",
        "print(\"=\" * 55)\n",
        "print(\"Model 1: Original float32 (253.20 MB)\")\n",
        "print(\"=\" * 55)\n",
        "model_original = DistilBertForQuestionAnswering.from_pretrained('federated_qa_model')\n",
        "answer = get_answer(model_original, inputs)\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {answer}\\n\")\n",
        "\n",
        "print(\"=\" * 55)\n",
        "print(\"Model 2: Dynamic INT8 (131.72 MB)\")\n",
        "print(\"=\" * 55)\n",
        "\n",
        "model_int8 = torch.quantization.quantize_dynamic(\n",
        "    DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased'),\n",
        "    {torch.nn.Linear},\n",
        "    dtype=torch.qint8\n",
        ")\n",
        "model_int8.load_state_dict(\n",
        "    torch.load('model_dynamic_int8.pt', map_location='cpu')\n",
        ")\n",
        "answer_int8 = get_answer(model_int8, inputs)\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {answer_int8}\\n\")\n",
        "\n",
        "print(\"=\" * 55)\n",
        "print(\"COMPARISON\")\n",
        "print(\"=\" * 55)\n",
        "print(f\"{'Model':<25} {'Size':>10}  {'Answer'}\")\n",
        "print(f\"{'-' * 53}\")\n",
        "print(f\"{'Original float32':<25} {'253.20 MB':>10}  {answer}\")\n",
        "print(f\"{'Dynamic INT8':<25} {'131.72 MB':>10}  {answer_int8}\")\n",
        "print(f\"\\nAnswers match: {'YES' if answer == answer_int8 else 'NO (slight difference)'}\")\n",
        "print(\"=\" * 55)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 882,
          "referenced_widgets": [
            "47c5ee9992594bcea466eb45b05fea9e",
            "149ac1bb5fbf411b80b17d5585e627ec",
            "e49db21ff00a4186a6d1d3edba1f823d",
            "025870d2dde24ab49961e4c73f1f08f9",
            "5b2c92e8295041cf8b69e3f359fa7fa5",
            "681b5fea7176400a931e58c333e91a9a",
            "594d70f49d6244688029dad389cf2f51",
            "a545652510a64910a7defecc6e30ddee",
            "501810a3f3b142cc879c4bd9f38e6cc8",
            "c924f3ca401c4884937e1dcf7fb1424f",
            "15bd3e2d127f486e9f0f6b4dd06e08a6",
            "09b172eac29b435998ce8bdf40824d19",
            "3b46ee776df44223a08816d1bdc57d74",
            "15169ebbfd514d499959ca1d047e5990",
            "66d64a34f40b46e18fd405161ef0974e",
            "57237dd874814aad88e0faa009e10c2b",
            "689d2f27528d438e9f7e9fc722a4fddc",
            "3dbc61739c5d4ed28299446846ccdd8d",
            "11c210abdcdd4ce19022f5c67d3a67a7",
            "5d52d3c07fcf4ea1ba1bfb4f8cd33345",
            "9e24419e3f324026816113a99e244951",
            "58b4d5d77ab9430aba0fa203bf858ffe"
          ]
        },
        "id": "8KB1AjxKP8UR",
        "outputId": "75b6b2ba-eefa-4a7a-a86d-510120a0d4b8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=======================================================\n",
            "Model 1: Original float32 (253.20 MB)\n",
            "=======================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/102 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "47c5ee9992594bcea466eb45b05fea9e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is Maria's specialty at the bakery?\n",
            "Answer: sourdough bread\n",
            "\n",
            "=======================================================\n",
            "Model 2: Dynamic INT8 (131.72 MB)\n",
            "=======================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "09b172eac29b435998ce8bdf40824d19"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DistilBertForQuestionAnswering LOAD REPORT from: distilbert-base-uncased\n",
            "Key                     | Status     | \n",
            "------------------------+------------+-\n",
            "vocab_transform.weight  | UNEXPECTED | \n",
            "vocab_transform.bias    | UNEXPECTED | \n",
            "vocab_projector.bias    | UNEXPECTED | \n",
            "vocab_layer_norm.weight | UNEXPECTED | \n",
            "vocab_layer_norm.bias   | UNEXPECTED | \n",
            "qa_outputs.bias         | MISSING    | \n",
            "qa_outputs.weight       | MISSING    | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
            "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n",
            "/tmp/ipython-input-4242178836.py:46: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  model_int8 = torch.quantization.quantize_dynamic(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_utils.py:444: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  device=storage.device,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is Maria's specialty at the bakery?\n",
            "Answer: sourdough bread\n",
            "\n",
            "=======================================================\n",
            "COMPARISON\n",
            "=======================================================\n",
            "Model                           Size  Answer\n",
            "-----------------------------------------------------\n",
            "Original float32           253.20 MB  sourdough bread\n",
            "Dynamic INT8               131.72 MB  sourdough bread\n",
            "\n",
            "Answers match: YES\n",
            "=======================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import DistilBertForQuestionAnswering, DistilBertTokenizerFast\n",
        "from datasets import load_dataset\n",
        "import collections\n",
        "import string\n",
        "import re\n",
        "\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "print(\"Loading SQuAD validation set...\")\n",
        "val_dataset = load_dataset('squad', split='validation')\n",
        "\n",
        "val_dataset = val_dataset.select(range(500))\n",
        "print(f\" {len(val_dataset)} examples loaded\\n\")\n",
        "\n",
        "\n",
        "def normalize_answer(s):\n",
        "    \"\"\"Lowercase, remove punctuation, articles, extra whitespace\"\"\"\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "    def remove_punctuation(text):\n",
        "        return ''.join(ch for ch in text if ch not in string.punctuation)\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "    return white_space_fix(remove_articles(remove_punctuation(s.lower())))\n",
        "\n",
        "def exact_match(prediction, ground_truths):\n",
        "    pred_norm = normalize_answer(prediction)\n",
        "    return any(pred_norm == normalize_answer(gt) for gt in ground_truths)\n",
        "\n",
        "def f1_score(prediction, ground_truths):\n",
        "    def token_f1(pred, gt):\n",
        "        pred_tokens = normalize_answer(pred).split()\n",
        "        gt_tokens   = normalize_answer(gt).split()\n",
        "        common      = collections.Counter(pred_tokens) & collections.Counter(gt_tokens)\n",
        "        num_same    = sum(common.values())\n",
        "        if num_same == 0:\n",
        "            return 0.0\n",
        "        precision = num_same / len(pred_tokens)\n",
        "        recall    = num_same / len(gt_tokens)\n",
        "        return (2 * precision * recall) / (precision + recall)\n",
        "    return max(token_f1(prediction, gt) for gt in ground_truths)\n",
        "\n",
        "def predict_answer(model, question, context):\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        question,\n",
        "        context,\n",
        "        return_tensors  = 'pt',\n",
        "        max_length      = 384,\n",
        "        truncation      = 'only_second',\n",
        "        padding         = 'max_length',\n",
        "        return_offsets_mapping = False\n",
        "    )\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs   = model(**inputs)\n",
        "        start_idx = torch.argmax(outputs.start_logits)\n",
        "        end_idx   = torch.argmax(outputs.end_logits)\n",
        "\n",
        "        # Guard: end must be >= start\n",
        "        if end_idx < start_idx:\n",
        "            end_idx = start_idx\n",
        "\n",
        "        tokens = inputs['input_ids'][0][start_idx : end_idx + 1]\n",
        "        answer = tokenizer.decode(tokens, skip_special_tokens=True)\n",
        "    return answer.strip()\n",
        "\n",
        "def evaluate_model(model, dataset, model_name):\n",
        "\n",
        "    print(f\"\\n Evaluating {model_name} on {len(dataset)} examples...\")\n",
        "\n",
        "    em_scores = []\n",
        "    f1_scores = []\n",
        "\n",
        "    for i, example in enumerate(dataset):\n",
        "        question     = example['question']\n",
        "        context      = example['context']\n",
        "        ground_truth = example['answers']['text']   # list of valid answers\n",
        "\n",
        "        prediction   = predict_answer(model, question, context)\n",
        "\n",
        "        em_scores.append(1.0 if exact_match(prediction, ground_truth) else 0.0)\n",
        "        f1_scores.append(f1_score(prediction, ground_truth))\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f\"  [{i+1}/{len(dataset)}] \"\n",
        "                  f\"EM: {np.mean(em_scores)*100:.2f}%  \"\n",
        "                  f\"F1: {np.mean(f1_scores)*100:.2f}%\")\n",
        "\n",
        "    final_em = np.mean(em_scores) * 100\n",
        "    final_f1 = np.mean(f1_scores) * 100\n",
        "    return final_em, final_f1\n",
        "\n",
        "# MODEL 1 — Original float32\n",
        "\n",
        "print(\"=\"*55)\n",
        "print(\"Model 1: Original float32 (253.20 MB)\")\n",
        "print(\"=\"*55)\n",
        "model_original = DistilBertForQuestionAnswering.from_pretrained('federated_qa_model')\n",
        "em1, f1_1 = evaluate_model(model_original, val_dataset, \"Original float32\")\n",
        "print(f\" Exact Match : {em1:.2f}%\")\n",
        "print(f\" F1 Score    : {f1_1:.2f}%\")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*55)\n",
        "print(\"Model 2: Dynamic INT8 (131.72 MB)\")\n",
        "print(\"=\"*55)\n",
        "model_int8 = torch.quantization.quantize_dynamic(\n",
        "    DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased'),\n",
        "    {torch.nn.Linear},\n",
        "    dtype=torch.qint8\n",
        ")\n",
        "model_int8.load_state_dict(\n",
        "    torch.load('model_dynamic_int8.pt', map_location='cpu')\n",
        ")\n",
        "em2, f1_2 = evaluate_model(model_int8, val_dataset, \"Dynamic INT8\")\n",
        "print(f\"Exact Match : {em2:.2f}%\")\n",
        "print(f\"F1 Score    : {f1_2:.2f}%\")\n",
        "\n",
        "# ========================================\n",
        "# 📊 FINAL COMPARISON\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*55)\n",
        "print(\"FINAL COMPARISON\")\n",
        "print(\"=\"*55)\n",
        "print(f\"  {'Model':<25} {'Size':>9}  {'EM':>8}  {'F1':>8}\")\n",
        "print(f\"  {'-'*53}\")\n",
        "print(f\"  {'Original float32':<25} {'253.20MB':>9}  {em1:>7.2f}%  {f1_1:>7.2f}%\")\n",
        "print(f\"  {'Dynamic INT8':<25} {'131.72MB':>9}  {em2:>7.2f}%  {f1_2:>7.2f}%\")\n",
        "print(f\"  {'-'*53}\")\n",
        "print(f\"  {'EM  drop':<25} {'':>9}  {em1-em2:>7.2f}%\")\n",
        "print(f\"  {'F1  drop':<25} {'':>9}  {f1_1-f1_2:>7.2f}%\")\n",
        "print(f\"  {'Size reduction':<25} {'1.9x':>9}\")\n",
        "print(\"=\"*55)\n",
        "\n",
        "if abs(f1_1 - f1_2) < 2.0:\n",
        "    print(\"\\n Quantization maintained accuracy — F1 drop < 2%\")\n",
        "else:\n",
        "    print(f\"\\n F1 dropped {f1_1-f1_2:.2f}% after quantization\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "69652f22ae224a3e88ea5d2146f48c8a",
            "cef8672849cf45508e8a8dfdfa553c96",
            "ee08ac14aa5a47deb3297446ac8a27c9",
            "59698524a0bf4e60bcbc928eb85cc064",
            "8f4e10e7a8604a22886bd74d3d56f5aa",
            "d69b2ec06d4a48c2b00fc8bcd5e2f9a3",
            "b61f60816b0543df951e15d0dd92a8ee",
            "85543c07ee234fdd93ea65c62d7010bf",
            "1c23a66f19f047b29456d7cbf1f8de83",
            "d3e078645de04c9cad377fbe1cd2693b",
            "add4a727ab1b48a88458b97bec74cdd9",
            "50e610e7ee6f4511b5eafaec4ae01a41",
            "f75786e7394b458389d2a5ef46f02123",
            "8eecb3c6532343fdb38c6f323cd5e381",
            "a04bdf280e2149179cf23210aff89839",
            "fb9e059419914ba1883ed469ba661f65",
            "ad1a3e9e09a24622b46a789ffd441e63",
            "a07d745afa70453b884253f95c576547",
            "64052ab0431a4088a32e9e0777fd4e7d",
            "52280b03e47a4beab958c8242b3abc43",
            "dc642e1153654db3977013993bd0ad51",
            "c56af0421af9468c8b0fd91937606a6b"
          ]
        },
        "id": "UsSLw7tjQO-r",
        "outputId": "d4448cf9-9a60-45fe-b378-245c2d5fbdad"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading SQuAD validation set...\n",
            " 500 examples loaded\n",
            "\n",
            "=======================================================\n",
            "Model 1: Original float32 (253.20 MB)\n",
            "=======================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/102 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "69652f22ae224a3e88ea5d2146f48c8a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Evaluating Original float32 on 500 examples...\n",
            "  [100/500] EM: 70.00%  F1: 78.88%\n",
            "  [200/500] EM: 72.00%  F1: 81.01%\n",
            "  [300/500] EM: 73.33%  F1: 79.68%\n",
            "  [400/500] EM: 73.50%  F1: 79.25%\n",
            "  [500/500] EM: 73.20%  F1: 79.12%\n",
            " Exact Match : 73.20%\n",
            " F1 Score    : 79.12%\n",
            "\n",
            "=======================================================\n",
            "Model 2: Dynamic INT8 (131.72 MB)\n",
            "=======================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "50e610e7ee6f4511b5eafaec4ae01a41"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DistilBertForQuestionAnswering LOAD REPORT from: distilbert-base-uncased\n",
            "Key                     | Status     | \n",
            "------------------------+------------+-\n",
            "vocab_transform.weight  | UNEXPECTED | \n",
            "vocab_transform.bias    | UNEXPECTED | \n",
            "vocab_projector.bias    | UNEXPECTED | \n",
            "vocab_layer_norm.weight | UNEXPECTED | \n",
            "vocab_layer_norm.bias   | UNEXPECTED | \n",
            "qa_outputs.bias         | MISSING    | \n",
            "qa_outputs.weight       | MISSING    | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
            "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n",
            "/tmp/ipython-input-3120760996.py:110: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  model_int8 = torch.quantization.quantize_dynamic(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Evaluating Dynamic INT8 on 500 examples...\n",
            "  [100/500] EM: 64.00%  F1: 75.51%\n",
            "  [200/500] EM: 66.00%  F1: 76.29%\n",
            "  [300/500] EM: 67.33%  F1: 74.86%\n",
            "  [400/500] EM: 69.00%  F1: 75.62%\n",
            "  [500/500] EM: 69.00%  F1: 75.75%\n",
            "Exact Match : 69.00%\n",
            "F1 Score    : 75.75%\n",
            "\n",
            "=======================================================\n",
            "FINAL COMPARISON\n",
            "=======================================================\n",
            "  Model                          Size        EM        F1\n",
            "  -----------------------------------------------------\n",
            "  Original float32           253.20MB    73.20%    79.12%\n",
            "  Dynamic INT8               131.72MB    69.00%    75.75%\n",
            "  -----------------------------------------------------\n",
            "  EM  drop                                4.20%\n",
            "  F1  drop                                3.37%\n",
            "  Size reduction                 1.9x\n",
            "=======================================================\n",
            "\n",
            " F1 dropped 3.37% after quantization\n"
          ]
        }
      ]
    }
  ]
}