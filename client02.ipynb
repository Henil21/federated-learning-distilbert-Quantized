{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader\nfrom transformers import (\n    DistilBertForQuestionAnswering,\n    DistilBertTokenizerFast\n)\nfrom datasets import load_dataset\nimport requests\nimport pickle\nimport base64\nimport gzip\nimport time\n\nRANDOM_SEED      = 42\nCLIENT_ID        = 2                         \nSERVER_URL       = \"http://bore.pub:64534\"    \n\nEPOCHS_PER_ROUND = 1\nMAX_ROUNDS       = 1\nBATCH_SIZE       = 8\nLR               = 3e-5\nMAX_LENGTH       = 384\nDOC_STRIDE       = 128\n\ntorch.manual_seed(RANDOM_SEED + CLIENT_ID)\nnp.random.seed(RANDOM_SEED + CLIENT_ID)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Device: {device}\")\n\nprint(\"Loading SQuAD dataset...\")\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\ndataset   = load_dataset('squad', split='train')\n\n# Client 2 gets SECOND half\nhalf    = len(dataset) // 2\ndataset = dataset.select(range(half, len(dataset)))   # ← second half\nprint(f\"Client {CLIENT_ID} — {len(dataset)} examples (second half)\\n\")\n\ndef preprocess(examples):\n    inputs = tokenizer(\n        examples['question'],\n        examples['context'],\n        max_length                = MAX_LENGTH,\n        truncation                = 'only_second',\n        stride                    = DOC_STRIDE,\n        return_overflowing_tokens = True,\n        return_offsets_mapping    = True,\n        padding                   = 'max_length'\n    )\n\n    offset_mapping  = inputs.pop('offset_mapping')\n    sample_map      = inputs.pop('overflow_to_sample_mapping')\n    answers         = examples['answers']\n    start_positions = []\n    end_positions   = []\n\n    for i, offset in enumerate(offset_mapping):\n        sample_idx   = sample_map[i]\n        answer       = answers[sample_idx]\n        cls_index    = inputs['input_ids'][i].index(tokenizer.cls_token_id)\n        sequence_ids = inputs.sequence_ids(i)\n\n        if len(answer['answer_start']) == 0:\n            start_positions.append(cls_index)\n            end_positions.append(cls_index)\n            continue\n\n        start_char = answer['answer_start'][0]\n        end_char   = start_char + len(answer['text'][0])\n\n        token_start = token_end = cls_index\n        for idx, (sid, (o_start, o_end)) in enumerate(zip(sequence_ids, offset)):\n            if sid != 1:\n                continue\n            if o_start <= start_char < o_end:\n                token_start = idx\n            if o_start < end_char <= o_end:\n                token_end = idx\n\n        start_positions.append(token_start)\n        end_positions.append(token_end)\n\n    inputs['start_positions'] = start_positions\n    inputs['end_positions']   = end_positions\n    return inputs\n\nprint(\"Tokenizing dataset (this takes ~2 min)...\")\ntokenized = dataset.map(\n    preprocess,\n    batched=True,\n    remove_columns=dataset.column_names\n)\ntokenized.set_format('torch')\nloader = DataLoader(tokenized, batch_size=BATCH_SIZE, shuffle=True)\nprint(f\"{len(tokenized)} tokenized examples ready\\n\")\n\ndef quantize_weights(state_dict):\n    quantized = {}\n    for key, tensor in state_dict.items():\n        w     = tensor.cpu().numpy().flatten().astype(np.float32)\n        w_min = float(np.min(w))\n        w_max = float(np.max(w))\n\n        if abs(w_max - w_min) < 1e-8:\n            scale = 1.0\n            zp    = 0.0\n            q     = np.zeros_like(w, dtype=np.int16)\n        else:\n            scale = (w_max - w_min) / 65535.0\n            zp    = w_min\n            q     = np.round((w - w_min) / scale).astype(np.int32) - 32768\n            q     = np.clip(q, -32768, 32767).astype(np.int16)\n\n        quantized[key] = {\n            'quantized'  : base64.b64encode(q.tobytes()).decode('utf-8'),\n            'scale'      : float(scale),\n            'zero_point' : float(zp),\n            'shape'      : list(tensor.shape),\n        }\n    return quantized\n\ndef get_size_mb(b):\n    return len(b) / (1024 * 1024)\n\n\ndef get_global_weights():\n    try:\n        response = requests.get(f\"{SERVER_URL}/get_weights\", timeout=60)\n        if response.status_code != 200:\n            print(f\"Server returned {response.status_code}\")\n            return None, None\n\n        data       = response.json()\n        compressed = base64.b64decode(data['weights'])\n        pickled    = gzip.decompress(compressed)\n        q_dict     = pickle.loads(pickled)\n\n        state_dict = {}\n        for key, item in q_dict.items():\n            q_bytes = base64.b64decode(item['quantized'])\n            q_array = np.frombuffer(q_bytes, dtype=np.int16).copy()\n            dq      = (q_array.astype(np.float32) + 32768) * item['scale'] + item['zero_point']\n            state_dict[key] = torch.tensor(dq.reshape(item['shape']))\n\n        return state_dict, data['round']\n\n    except Exception as e:\n        print(f\"Error getting weights: {e}\")\n        return None, None\n\ndef submit_weights(state_dict, round_num):\n    try:\n        original_size = get_size_mb(pickle.dumps(\n            {k: v.cpu().numpy() for k, v in state_dict.items()}\n        ))\n\n        print(\"      Quantizing..\")\n        q_dict     = quantize_weights(state_dict)\n        pickled    = pickle.dumps(q_dict)\n        compressed = gzip.compress(pickled, compresslevel=6)\n        encoded    = base64.b64encode(compressed).decode('utf-8')\n        comp_size  = get_size_mb(compressed)\n\n        print(f\"   {original_size:.1f} MB → {comp_size:.1f} MB \"\n              f\"({original_size/comp_size:.1f}x smaller)\")\n\n        payload = {\n            'client_id'         : CLIENT_ID,\n            'weights'           : encoded,\n            'round'             : round_num,\n            'original_size_mb'  : original_size,\n            'compressed_size_mb': comp_size,\n        }\n\n        response = requests.post(\n            f\"{SERVER_URL}/submit_weights\",\n            json=payload,\n            timeout=120\n        )\n\n        if response.status_code != 200:\n            print(f\"Server returned {response.status_code}: {response.text[:200]}\")\n            return None\n        return response.json()\n\n    except Exception as e:\n        print(f\"Error submitting weights: {e}\")\n        import traceback; traceback.print_exc()\n        return None\n\nmodel = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased')\nmodel.to(device)\n\nprint(f\"  Federated QA Training — Client {CLIENT_ID}\")\nprint(f\"   Server : {SERVER_URL}\")\nprint(f\"   Rounds : {MAX_ROUNDS}  |  Epochs/round: {EPOCHS_PER_ROUND}\\n\")\n\nfor round_num in range(MAX_ROUNDS):\n    print(f\"\\n{'='*60}\")\n    print(f\"  ROUND {round_num+1}/{MAX_ROUNDS} — Client {CLIENT_ID}\")\n    print(f\"{'='*60}\")\n\n    print(\"Downloading global model...\")\n    state_dict, server_round = get_global_weights()\n    if state_dict is None:\n        print(\"Failed — retrying in 10s...\")\n        time.sleep(10)\n        continue\n    model.load_state_dict(state_dict)\n    print(\"Global model loaded\")\n\n    print(f\"Training ({EPOCHS_PER_ROUND} epochs)...\")\n    model.train()\n    optimizer = AdamW(model.parameters(), lr=LR)\n\n    for epoch in range(EPOCHS_PER_ROUND):\n        total_loss = 0\n        for step, batch in enumerate(loader):\n            batch    = {k: v.to(device) for k, v in batch.items()}\n            outputs  = model(**batch)\n            loss     = outputs.loss\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            total_loss += loss.item()\n\n            if step % 50 == 0:\n                print(f\"   Epoch {epoch+1} | Step {step}/{len(loader)} \"\n                      f\"| Loss: {loss.item():.4f}\")\n\n        avg_loss = total_loss / len(loader)\n        print(f\"Epoch {epoch+1} — Avg Loss: {avg_loss:.4f}\")\n\n    print(\"Uploading weights...\")\n    result = submit_weights(model.state_dict(), round_num)\n\n    if result and result.get('status') == 'success':\n        print(\"Weights submitted successfully\")\n        if result.get('aggregating'):\n            print(\"Server aggregating...\")\n    else:\n        print(\"Failed to submit weights\")\n\n    time.sleep(5)\n\nprint(f\"\\n Client {CLIENT_ID} training complete!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-13T14:46:12.838869Z","iopub.execute_input":"2026-02-13T14:46:12.839191Z","iopub.status.idle":"2026-02-13T15:16:33.093900Z","shell.execute_reply.started":"2026-02-13T14:46:12.839161Z","shell.execute_reply":"2026-02-13T15:16:33.093218Z"}},"outputs":[{"name":"stderr","text":"2026-02-13 14:46:34.207311: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1770993994.639053      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1770993994.767529      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1770993995.681350      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770993995.681397      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770993995.681400      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770993995.681403      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"Device: cuda\nLoading SQuAD dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f95bf47f5d1e45249f5f2fe0045ec3d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d0222caa2664cd394101694f8d01731"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6278aa935e444a568ea5110bc8b32ffc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13067b144f92456d839635dc9052d26a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d93d1ead3d144872a821a653d369a329"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"plain_text/train-00000-of-00001.parquet:   0%|          | 0.00/14.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ff840a595964b92bbd074ba26db642f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"plain_text/validation-00000-of-00001.par(…):   0%|          | 0.00/1.82M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b45d22142f94b91a3fa34fdf952ffe4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"450f4e6013a0400b9df4b350a4f2d22b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cb2db7cb628466cbe8dfc1485126f62"}},"metadata":{}},{"name":"stdout","text":"Client 2 — 43800 examples (second half)\n\nTokenizing dataset (this takes ~2 min)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/43800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e67f0a58ce341379c36cb794193294c"}},"metadata":{}},{"name":"stdout","text":"44319 tokenized examples ready\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c59d6991bf345f0952bd3fa47265773"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"  Federated QA Training — Client 2\n   Server : http://bore.pub:64534\n   Rounds : 1  |  Epochs/round: 1\n\n\n============================================================\n  ROUND 1/1 — Client 2\n============================================================\nDownloading global model...\nGlobal model loaded\nTraining (1 epochs)...\n   Epoch 1 | Step 0/5540 | Loss: 5.9401\n   Epoch 1 | Step 50/5540 | Loss: 3.6917\n   Epoch 1 | Step 100/5540 | Loss: 3.7409\n   Epoch 1 | Step 150/5540 | Loss: 4.0092\n   Epoch 1 | Step 200/5540 | Loss: 2.9883\n   Epoch 1 | Step 250/5540 | Loss: 1.8115\n   Epoch 1 | Step 300/5540 | Loss: 2.8549\n   Epoch 1 | Step 350/5540 | Loss: 2.3659\n   Epoch 1 | Step 400/5540 | Loss: 1.4138\n   Epoch 1 | Step 450/5540 | Loss: 3.0196\n   Epoch 1 | Step 500/5540 | Loss: 2.8293\n   Epoch 1 | Step 550/5540 | Loss: 3.7957\n   Epoch 1 | Step 600/5540 | Loss: 1.4699\n   Epoch 1 | Step 650/5540 | Loss: 3.1806\n   Epoch 1 | Step 700/5540 | Loss: 2.6292\n   Epoch 1 | Step 750/5540 | Loss: 1.8651\n   Epoch 1 | Step 800/5540 | Loss: 2.0066\n   Epoch 1 | Step 850/5540 | Loss: 2.0473\n   Epoch 1 | Step 900/5540 | Loss: 2.7684\n   Epoch 1 | Step 950/5540 | Loss: 2.3285\n   Epoch 1 | Step 1000/5540 | Loss: 2.0955\n   Epoch 1 | Step 1050/5540 | Loss: 0.9577\n   Epoch 1 | Step 1100/5540 | Loss: 2.3587\n   Epoch 1 | Step 1150/5540 | Loss: 1.4213\n   Epoch 1 | Step 1200/5540 | Loss: 1.7769\n   Epoch 1 | Step 1250/5540 | Loss: 2.0587\n   Epoch 1 | Step 1300/5540 | Loss: 1.4249\n   Epoch 1 | Step 1350/5540 | Loss: 1.2741\n   Epoch 1 | Step 1400/5540 | Loss: 1.0555\n   Epoch 1 | Step 1450/5540 | Loss: 1.1136\n   Epoch 1 | Step 1500/5540 | Loss: 1.8760\n   Epoch 1 | Step 1550/5540 | Loss: 1.8714\n   Epoch 1 | Step 1600/5540 | Loss: 1.9257\n   Epoch 1 | Step 1650/5540 | Loss: 1.1370\n   Epoch 1 | Step 1700/5540 | Loss: 1.3666\n   Epoch 1 | Step 1750/5540 | Loss: 1.4219\n   Epoch 1 | Step 1800/5540 | Loss: 1.4073\n   Epoch 1 | Step 1850/5540 | Loss: 0.8983\n   Epoch 1 | Step 1900/5540 | Loss: 1.7911\n   Epoch 1 | Step 1950/5540 | Loss: 0.7248\n   Epoch 1 | Step 2000/5540 | Loss: 1.9761\n   Epoch 1 | Step 2050/5540 | Loss: 1.4635\n   Epoch 1 | Step 2100/5540 | Loss: 1.5790\n   Epoch 1 | Step 2150/5540 | Loss: 3.0107\n   Epoch 1 | Step 2200/5540 | Loss: 2.1777\n   Epoch 1 | Step 2250/5540 | Loss: 1.4267\n   Epoch 1 | Step 2300/5540 | Loss: 1.2456\n   Epoch 1 | Step 2350/5540 | Loss: 1.3185\n   Epoch 1 | Step 2400/5540 | Loss: 1.1073\n   Epoch 1 | Step 2450/5540 | Loss: 0.8183\n   Epoch 1 | Step 2500/5540 | Loss: 1.3666\n   Epoch 1 | Step 2550/5540 | Loss: 2.1353\n   Epoch 1 | Step 2600/5540 | Loss: 1.7607\n   Epoch 1 | Step 2650/5540 | Loss: 1.2730\n   Epoch 1 | Step 2700/5540 | Loss: 1.7316\n   Epoch 1 | Step 2750/5540 | Loss: 2.2175\n   Epoch 1 | Step 2800/5540 | Loss: 1.8100\n   Epoch 1 | Step 2850/5540 | Loss: 0.8253\n   Epoch 1 | Step 2900/5540 | Loss: 1.2672\n   Epoch 1 | Step 2950/5540 | Loss: 1.7929\n   Epoch 1 | Step 3000/5540 | Loss: 1.5805\n   Epoch 1 | Step 3050/5540 | Loss: 0.5725\n   Epoch 1 | Step 3100/5540 | Loss: 1.1465\n   Epoch 1 | Step 3150/5540 | Loss: 1.5866\n   Epoch 1 | Step 3200/5540 | Loss: 1.0325\n   Epoch 1 | Step 3250/5540 | Loss: 2.0100\n   Epoch 1 | Step 3300/5540 | Loss: 1.5052\n   Epoch 1 | Step 3350/5540 | Loss: 2.0764\n   Epoch 1 | Step 3400/5540 | Loss: 1.2433\n   Epoch 1 | Step 3450/5540 | Loss: 0.9935\n   Epoch 1 | Step 3500/5540 | Loss: 0.6711\n   Epoch 1 | Step 3550/5540 | Loss: 1.4609\n   Epoch 1 | Step 3600/5540 | Loss: 2.1415\n   Epoch 1 | Step 3650/5540 | Loss: 2.4975\n   Epoch 1 | Step 3700/5540 | Loss: 1.4036\n   Epoch 1 | Step 3750/5540 | Loss: 1.4529\n   Epoch 1 | Step 3800/5540 | Loss: 1.2892\n   Epoch 1 | Step 3850/5540 | Loss: 1.4718\n   Epoch 1 | Step 3900/5540 | Loss: 0.9170\n   Epoch 1 | Step 3950/5540 | Loss: 1.4293\n   Epoch 1 | Step 4000/5540 | Loss: 1.4198\n   Epoch 1 | Step 4050/5540 | Loss: 0.7756\n   Epoch 1 | Step 4100/5540 | Loss: 0.7315\n   Epoch 1 | Step 4150/5540 | Loss: 1.2322\n   Epoch 1 | Step 4200/5540 | Loss: 1.5167\n   Epoch 1 | Step 4250/5540 | Loss: 1.6111\n   Epoch 1 | Step 4300/5540 | Loss: 1.0437\n   Epoch 1 | Step 4350/5540 | Loss: 1.0419\n   Epoch 1 | Step 4400/5540 | Loss: 1.5939\n   Epoch 1 | Step 4450/5540 | Loss: 1.0061\n   Epoch 1 | Step 4500/5540 | Loss: 1.6412\n   Epoch 1 | Step 4550/5540 | Loss: 2.7129\n   Epoch 1 | Step 4600/5540 | Loss: 1.6582\n   Epoch 1 | Step 4650/5540 | Loss: 1.6953\n   Epoch 1 | Step 4700/5540 | Loss: 0.7773\n   Epoch 1 | Step 4750/5540 | Loss: 1.0930\n   Epoch 1 | Step 4800/5540 | Loss: 1.0562\n   Epoch 1 | Step 4850/5540 | Loss: 0.8397\n   Epoch 1 | Step 4900/5540 | Loss: 1.3268\n   Epoch 1 | Step 4950/5540 | Loss: 1.3126\n   Epoch 1 | Step 5000/5540 | Loss: 1.5822\n   Epoch 1 | Step 5050/5540 | Loss: 0.5946\n   Epoch 1 | Step 5100/5540 | Loss: 1.0550\n   Epoch 1 | Step 5150/5540 | Loss: 1.4756\n   Epoch 1 | Step 5200/5540 | Loss: 2.5071\n   Epoch 1 | Step 5250/5540 | Loss: 1.5373\n   Epoch 1 | Step 5300/5540 | Loss: 2.0551\n   Epoch 1 | Step 5350/5540 | Loss: 0.9726\n   Epoch 1 | Step 5400/5540 | Loss: 1.5256\n   Epoch 1 | Step 5450/5540 | Loss: 1.0653\n   Epoch 1 | Step 5500/5540 | Loss: 1.2933\nEpoch 1 — Avg Loss: 1.7204\nUploading weights...\n      Quantizing..\n   253.2 MB → 124.0 MB (2.0x smaller)\nWeights submitted successfully\n\n Client 2 training complete!\n","output_type":"stream"}],"execution_count":1}]}